{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5.2 Assignment: LSTM Sequence Completion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Daniel Nascimento  -  danielsdn0725@gmail.com"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1 - Modules and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan-dan/miniconda3/envs/molude5/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "import os\n",
        "import random as rnd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import string\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  The file input.txt - is my root, if you need to salve it in a different folder, please informa here:\n",
        "# root_folder=''\n",
        "# data_folder_name='./'\n",
        "# model_folder_name=''\n",
        "\n",
        "root_folder=''\n",
        "data_folder_name='./data'\n",
        "model_folder_name=''\n",
        "\n",
        "filename='input.txt'\n",
        "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
        "model_dir = os.path.abspath(os.path.join(root_folder, model_folder_name))\n",
        "train_path = os.path.join(DATA_PATH, filename)\n",
        "\n",
        "seed = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_text_data(filename, init_dialog=False):\n",
        "    ''' Load the texts from the filename, splitting by lines and removing empty strings.\n",
        "        Setting init_dialog = True will remove lines where the character who is going to speak is indicated\n",
        "    '''\n",
        "    sentences = []\n",
        "    with open(filename, 'r') as reader:\n",
        "      for line in reader:\n",
        "            if init_dialog or ':' not in line:\n",
        "                sentences.append(line[:-1])\n",
        "                \n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(sentences, alpha=False):\n",
        "    ''' Cleaning process of the text'''\n",
        "    if alpha:\n",
        "        # Remove non alphabetic character\n",
        "        cleaned_text = [''.join([t.lower() for t in text if t.isalpha() or t.isspace()]) for text in sentences]\n",
        "    else:\n",
        "        # Simply lower the characters\n",
        "        cleaned_text = [t.lower() for t in sentences]\n",
        "    # Remove any emoty string\n",
        "    cleaned_text = [t for t in cleaned_text if t!='']\n",
        "    \n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "class CharVocab: \n",
        "    ''' Create a Vocabulary for '''\n",
        "    def __init__(self, type_vocab,pad_token='<PAD>', eos_token='<EOS>', unk_token='<UNK>'): #Initialization of the type of vocabulary\n",
        "        self.type = type_vocab\n",
        "        #self.int2char ={}\n",
        "        self.int2char = []\n",
        "        if pad_token !=None:\n",
        "            self.int2char += [pad_token]\n",
        "        if eos_token !=None:\n",
        "            self.int2char += [eos_token]\n",
        "        if unk_token !=None:\n",
        "            self.int2char += [unk_token]\n",
        "        self.char2int = {}\n",
        "        \n",
        "    def __call__(self, text):       \n",
        "        chars = set(''.join(text))\n",
        "\n",
        "        self.int2char += list(chars)\n",
        "        self.char2int = {char: ind for ind, char in enumerate(self.int2char)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences:  29723\n",
            "['Before we proceed any further, hear me speak.', '', 'Speak, speak.', '', 'You are all resolved rather to die than to famish?', '', 'Resolved. resolved.', '', 'First, you know Caius Marcius is chief enemy to the people.', '', \"We know't, we know't.\", '', \"Let us kill him, and we'll have corn at our own price.\", \"Is't a verdict?\", '', '', 'One word, good citizens.', '', 'We are accounted poor citizens, the patricians good.', 'would yield us but the superfluity, while it were']\n"
          ]
        }
      ],
      "source": [
        "sentences = load_text_data(train_path)\n",
        "print('Number of sentences: ', len(sentences))\n",
        "print(sentences[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of characters:  894875\n",
            "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than to famish? resolved. resolved. first, you know caius marcius is chief enemy to the people. we know't\n"
          ]
        }
      ],
      "source": [
        "# Clean the sentences\n",
        "sentences = clean_text(sentences, False)\n",
        "# entences in a one long string\n",
        "sentences = ' '.join(sentences)\n",
        "print('Number of characters: ', len(sentences))\n",
        "print(sentences[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = CharVocab('char',None,None,'<UNK>')\n",
        "vocab(sentences)\n",
        "#print('Length of vocabulary: ', len(vocab.int2char))\n",
        "#print('Int to Char: ', vocab.int2char)\n",
        "#print('Char to Int: ', vocab.char2int)\n",
        "\n",
        "\n",
        "if not os.path.exists(DATA_PATH): \n",
        "    os.makedirs(DATA_PATH)\n",
        "    \n",
        "\n",
        "with open(os.path.join(DATA_PATH, 'char_dict.pkl'), \"wb\") as f:\n",
        "    pickle.dump(vocab.char2int, f)\n",
        "\n",
        "with open(os.path.join(DATA_PATH, 'int_dict.pkl'), \"wb\") as f:\n",
        "    pickle.dump(vocab.int2char, f)\n",
        "    \n",
        "    \n",
        "# Check or create the directory \n",
        "if not os.path.exists(DATA_PATH): # in you environment, you have this path\n",
        "    os.makedirs(DATA_PATH)\n",
        "    \n",
        "# Save the dictionary to data path dir  \n",
        "with open(os.path.join(DATA_PATH, 'char_dict.pkl'), \"wb\") as f:\n",
        "    pickle.dump(vocab.char2int, f)\n",
        "\n",
        "with open(os.path.join(DATA_PATH, 'int_dict.pkl'), \"wb\") as f:\n",
        "    pickle.dump(vocab.int2char, f)    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2 - Encoding the dataset and batch creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encode(indices, dict_size):\n",
        "    ''' Define one hot encode matrix for our sequences'''\n",
        "    # Creating a multi-dimensional array with the desired output shape\n",
        "    # Encode every integer with its one hot representation\n",
        "    features = np.eye(dict_size, dtype=np.float32)[indices.flatten()]\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    features = features.reshape((*indices.shape, dict_size))\n",
        "            \n",
        "    return features\n",
        "\n",
        "def encode_text(input_text, vocab, one_hot = False):\n",
        "    ''' Encode the input_text replacing the char by its integer number based on the dictionary vocab'''\n",
        "    # Replace every char by its integer value based on the vocabulary\n",
        "    output = [vocab.char2int.get(character,0) for character in input_text]\n",
        "    \n",
        "    if one_hot:\n",
        "    # One hot encode every integer of the sequence\n",
        "        dict_size = len(vocab.char2int)\n",
        "        return one_hot_encode(output, dict_size)\n",
        "    else:\n",
        "        return np.array(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode the train dataset\n",
        "train_data = encode_text(sentences, vocab, one_hot = False)\n",
        "\n",
        "# Create the input sequence, from 0 to len-1\n",
        "input_seq=train_data[:-1]\n",
        "# Create the target sequence, from 1 to len. It is right-shifted one place\n",
        "target_seq=train_data[1:]\n",
        "#print('\\nOriginal text:')\n",
        "#print(sentences[:100000])\n",
        "#print('\\nEncoded text:')\n",
        "#print(train_data[:100000])\n",
        "#print('\\nInput sequence:')\n",
        "#print(input_seq[:100000])\n",
        "#print('\\nTarget sequence:')\n",
        "#print(target_seq[:100000])\n",
        "\n",
        "# Save the encoded text to a file\n",
        "encoded_data = os.path.join(DATA_PATH, 'input_data.pkl')\n",
        "with open(encoded_data, 'wb') as fp:\n",
        "    pickle.dump(train_data, fp)\n",
        "    \n",
        "    \n",
        "    \n",
        "print('Encoded characters: ',train_data[100:102])\n",
        "print('One-hot-encoded characters: ',one_hot_encode(train_data[100:102], 100))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_generator_sequence(features_seq, label_seq, batch_size, seq_len):\n",
        "    \"\"\"Generator function that yields batches of data (input and target)\n",
        "\n",
        "    Args:\n",
        "        features_seq: sequence of chracters, feature of our model.\n",
        "        label_seq: sequence of chracters, the target label of our model\n",
        "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "        seq_len (int): maximum length of the output tensor.\n",
        "\n",
        "    Yields:\n",
        "        x_epoch: sequence of features for the epoch\n",
        "        y_epoch: sequence of labels for the epoch\n",
        "    \"\"\"\n",
        "    # calculate the number of batches\n",
        "    num_batches = len(features_seq) // (batch_size * seq_len)\n",
        "    if num_batches == 0:\n",
        "        raise ValueError(\"No batches created. Use smaller batch size or sequence length.\")\n",
        "\n",
        "    rounded_len = num_batches * batch_size * seq_len\n",
        "    x = np.reshape(features_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
        "    y = np.reshape(label_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
        "    \n",
        "    epoch = 0\n",
        "    while True:\n",
        "        x_epoch = np.split(np.roll(x, -epoch, axis=0), num_batches, axis=1)\n",
        "        y_epoch = np.split(np.roll(y, -epoch, axis=0), num_batches, axis=1)\n",
        "        for batch in range(num_batches):\n",
        "            yield x_epoch[batch], y_epoch[batch]\n",
        "        epoch += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3 - Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_dim, n_layers, drop_rate=0.2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.drop_rate = drop_rate\n",
        "        self.char2int = None\n",
        "        self.int2char = None    \n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        # LSTM Layer\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_dim, n_layers, dropout=drop_rate, batch_first = True)\n",
        "        # Fully connected layer\n",
        "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, x, state):\n",
        "        rnn_out, state = self.rnn(x, state)\n",
        "        rnn_out = self.dropout(rnn_out)\n",
        "        rnn_out = rnn_out.contiguous().view(-1, self.hidden_dim)\n",
        "        logits = self.decoder(rnn_out)\n",
        "\n",
        "        #print('Output model shape: ', logits.shape)\n",
        "        return logits, state\n",
        "    \n",
        "    def init_state(self, device, batch_size=1):\n",
        "        \"\"\"\n",
        "        initialises rnn states.\n",
        "        \"\"\"\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
        "\n",
        "    def predict(self, input):\n",
        "        logits, hidden = self.forward(input)\n",
        "        probs = F.softmax(logits)\n",
        "        probs = probs.view(input.size(0), input.size(1), probs.size(1))\n",
        "        return probs, hidden"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4 - Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_main(model, optimizer, loss_fn, batch_data, num_batches, val_batches, batch_size, seq_len, n_epochs, clip_norm, device):\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        epoch_losses = []\n",
        "        hidden = model.init_state(device, batch_size)\n",
        "        for i in tqdm(range(num_batches-val_batches), desc=\"Epoch {}/{}\".format(epoch, n_epochs+1)):\n",
        "            input_batch, target_batch = next(batch_data)\n",
        "            input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
        "            input_data = torch.from_numpy(input_batch)\n",
        "            target_data = torch.from_numpy(target_batch)\n",
        "\n",
        "            hidden = tuple(([Variable(var.data) for var in hidden]))\n",
        "            input_data = input_data.to(device)\n",
        "\n",
        "            model.train()\n",
        "            optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "            output, hidden = model(input_data, hidden)\n",
        "            output = output.to(device)\n",
        "            target_data = target_data.to(device)\n",
        "            target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
        "            loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
        "            epoch_losses.append(loss.item()) #data[0]\n",
        "        \n",
        "            loss.backward() # backpropagation and  gradients\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
        "            \n",
        "            optimizer.step() # Updates the weights \n",
        "    \n",
        "        model.eval()\n",
        "        val_hidden = model.init_state(device, batch_size)\n",
        "        val_losses = []\n",
        "        for i in tqdm(range(val_batches), desc=\"Val Epoch {}/{}\".format(epoch, n_epochs+1)):\n",
        "            input_batch, target_batch = next(batch_data)\n",
        "            input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
        "            input_data = torch.from_numpy(input_batch)\n",
        "            target_data = torch.from_numpy(target_batch)\n",
        "            hidden = tuple(([Variable(var.data) for var in val_hidden]))\n",
        "            input_data = input_data.to(device)\n",
        "            output, hidden = model(input_data, hidden)\n",
        "            output = output.to(device)\n",
        "            target_data = target_data.to(device)\n",
        "            target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
        "            loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "        model.train()                  \n",
        "        #if epoch%2 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Train Loss: {:.4f}\".format(np.mean(epoch_losses)), end=' ')\n",
        "        print(\"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "        \n",
        "    return epoch_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f93de3c8ed0>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hyperparameters for training\n",
        "n_epochs = 50\n",
        "lr=0.001\n",
        "batch_size=64\n",
        "maxlen=64\n",
        "clip_norm=5\n",
        "val_fraction = 0.1\n",
        "\n",
        "\n",
        "hidden_dim = 64 #64\n",
        "n_layers = 1\n",
        "embedding_size=len(vocab.char2int)\n",
        "dict_size = len(vocab.char2int)\n",
        "drop_rate = 0.2\n",
        "\n",
        "# Set the device for training\n",
        "print('Device: ', device)\n",
        "# Set a seed to reproduce experiments\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNModel(\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (rnn): LSTM(38, 64, batch_first=True, dropout=0.2)\n",
            "  (decoder): Linear(in_features=64, out_features=38, bias=True)\n",
            ")\n",
            "100000\n"
          ]
        }
      ],
      "source": [
        "model = RNNModel(dict_size,embedding_size, hidden_dim, n_layers).to(device)\n",
        "print(model)\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "input_seq = input_seq[:100000]\n",
        "target_seq = target_seq[:100000]\n",
        "print(len(input_seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the number of batches to train\n",
        "num_batches = len(input_seq) // (batch_size*maxlen)\n",
        "val_batches = int(num_batches*val_fraction)\n",
        "# Create the batch data generator\n",
        "batch_data = batch_generator_sequence(input_seq, target_seq, batch_size, maxlen)\n",
        "losses = train_main(model, optimizer, criterion, batch_data, num_batches, val_batches, batch_size, \n",
        "                    maxlen, n_epochs, clip_norm, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the param\n",
        "model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
        "with open(model_info_path, 'wb') as f:\n",
        "    model_info = {\n",
        "        'n_layers': n_layers,\n",
        "        'embedding_dim': embedding_size,\n",
        "        'hidden_dim': hidden_dim,\n",
        "        'vocab_size': dict_size,\n",
        "        'drop_rate': drop_rate\n",
        "    }\n",
        "    torch.save(model_info, f)\n",
        "\n",
        "# Save the model param\n",
        "model_path = os.path.join(model_dir, 'model.pth')\n",
        "with open(model_path, 'wb') as f:\n",
        "    torch.save(model.state_dict(), f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5 - Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_from_probs(probs, top_n=10):\n",
        "    \"\"\"\n",
        "    truncated weighted random choice.\n",
        "    \"\"\"\n",
        "    _, indices = torch.sort(probs)\n",
        "    probs[indices.data[:-top_n]] = 0\n",
        "    sampled_index = torch.multinomial(probs, 1)\n",
        "    return sampled_index\n",
        "\n",
        "def predict_probs(model, hidden, character, vocab):\n",
        "    character = np.array([[vocab.char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, model.vocab_size)\n",
        "    character = torch.from_numpy(character)\n",
        "    character = character.to(device)\n",
        "    \n",
        "    out, hidden = model(character, hidden)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "\n",
        "    return prob, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_from_text(model, out_len, vocab, top_n=1, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    state = model.init_state(device, 1)\n",
        "    \n",
        "    for ch in chars:\n",
        "        probs, state = predict_probs(model, state, ch, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "\n",
        "    for ii in range(size):\n",
        "        probs, state = predict_probs(model, state, chars, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        chars.append(vocab.int2char[next_index.data[0]])\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "antonio wants a coffee has the\n",
            "30\n"
          ]
        }
      ],
      "source": [
        "text_predicted = generate_from_text(model, 30, vocab, 3, 'Antonio wants a coffee ')\n",
        "print(text_predicted)\n",
        "print(len(text_predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_from_char(model, out_len, vocab, top_n=1, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    state = model.init_state(device, 1)\n",
        "    for ch in chars:\n",
        "        probs, state = predict_probs(model, state, ch, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        \n",
        "    chars.append(vocab.int2char[next_index.data[0]])   \n",
        "    \n",
        "    for ii in range(size-1):\n",
        "        probs, state = predict_probs(model, state, chars[-1], vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        chars.append(vocab.int2char[next_index.data[0]])\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "antonio wants a coffee prepared by t\n",
            "36\n"
          ]
        }
      ],
      "source": [
        "text_predicted = generate_from_char(model, 30, vocab, 3, 'Antonio wants a coffee prepared by ')\n",
        "print(text_predicted)\n",
        "print(len(text_predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_char(model, character, vocab):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[vocab.char2int[c] for c in character]])\n",
        "    #character = one_hot_encode(character, len(vocab.char2int), character.shape[1], 1)\n",
        "    character = one_hot_encode(character, model.vocab_size)\n",
        "    character = torch.from_numpy(character)\n",
        "    # Generate set the device\n",
        "    character = character.to(device)\n",
        "    \n",
        "    model.eval() # eval mode\n",
        "    # Generate the initial hidden state\n",
        "    state = model.init_state(device, 1)\n",
        "\n",
        "    out, hidden = model(character, state)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    m = torch.max(prob, dim=0)\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return vocab.int2char[char_ind], hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial string:  t\n"
          ]
        }
      ],
      "source": [
        "t,_ = predict_char(model, 'we want a coffee servide by ', vocab)\n",
        "print('Initial string: ', t)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "molude5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "11f7695b1d0382e450fdaf4bb6c1b92eac8dc32696730d4c53196263c5d38a13"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
